{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19EC10086_A1_NGRAM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **N-gram Language model**\n",
        "[ 19EC10086 Shruti Shreyasi ]\n",
        "\n",
        "Run on Google Colaboratory"
      ],
      "metadata": {
        "id": "Jd_HDNFP6tHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary imports"
      ],
      "metadata": {
        "id": "FTxSFOVk7IVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cmath import exp, log"
      ],
      "metadata": {
        "id": "9XPuDLLN6sdL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language Model Class Definition"
      ],
      "metadata": {
        "id": "G5WA7_G77NwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, N, corpus, test_corpus):\n",
        "        '''\n",
        "            initialization of the class\n",
        "            args\n",
        "                N :: is the number corresponding to the N gram\n",
        "                corpus :: is the train dataset\n",
        "                test_corpus :: is the test dataset\n",
        "        '''\n",
        "        self.N = N\n",
        "        self.corpus = corpus\n",
        "        self.test_corpus = test_corpus\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        '''\n",
        "            i) add start and end tokens to the dataset:\n",
        "                    unigrams: start and end tokens are not added because they have no effect\n",
        "                    other n-grams: add N-1 start tokens and 1 end token\n",
        "            ii) words occuring only once are treated as UNK\n",
        "        '''\n",
        "        self.count_test_corpus = 0              # keep track of test corpus size\n",
        "\n",
        "        # i) add start and end tokens to the dataset\n",
        "        add_start_token = ''                    # keep track of start tokens\n",
        "        for i in range(self.N-1):\n",
        "            add_start_token += '<START> '\n",
        "        add_end_token = ' <END>'                # keep track of end tokens\n",
        "        if self.N == 1:\n",
        "            add_end_token = ''\n",
        "        # add start and end tokens\n",
        "        for i in range(len(self.corpus)):\n",
        "            self.corpus[i] = add_start_token + self.corpus[i] + add_end_token\n",
        "        for i in range(len(self.test_corpus)):\n",
        "            self.test_corpus[i] = add_start_token + self.test_corpus[i] + add_end_token\n",
        "\n",
        "        # ii) words occuring only once are treated as UNK\n",
        "        for sentence in self.test_corpus:\n",
        "            self.count_test_corpus += len(sentence.split())\n",
        "        vocabulary_before_seeding = {}         # keep track of tokens occuring only once\n",
        "        for sentence in self.corpus:\n",
        "            for word in sentence.split():\n",
        "                if word in vocabulary_before_seeding:\n",
        "                    vocabulary_before_seeding[word] += 1\n",
        "                else:\n",
        "                    vocabulary_before_seeding[word] = 1\n",
        "        # remove words with freq 1 from train corpus\n",
        "        for i in range(len(self.corpus)):\n",
        "            sentence = self.corpus[i]\n",
        "            sent = ''\n",
        "            for word in sentence.split():\n",
        "                if vocabulary_before_seeding[word] <= 1:\n",
        "                    sent += '<UNK> '\n",
        "                else:\n",
        "                    sent += word + ' '\n",
        "            sent = sent[:len(sent)-1]\n",
        "            self.corpus[i] = sent\n",
        "        # remove words with freq 1/0 in train set from test corpus\n",
        "        for i in range(len(self.test_corpus)):\n",
        "            sentence = self.test_corpus[i]\n",
        "            sent = ''\n",
        "            for word in sentence.split():\n",
        "                if word not in vocabulary_before_seeding or vocabulary_before_seeding[word] <= 1:\n",
        "                    sent += '<UNK> '\n",
        "                else:\n",
        "                    sent += word + ' '\n",
        "            sent = sent[:len(sent)-1]\n",
        "            self.test_corpus[i] = sent\n",
        "            \n",
        "    def n_gram_model(self):\n",
        "        '''\n",
        "            model definition having n gram and n-1 gram\n",
        "            i) store frequencies of all n-grams and n-1 grams\n",
        "        '''\n",
        "        self.preprocess_data()\n",
        "        self.count_total_unique_n_grams = 0            # keep track of total number of unique n grams\n",
        "        self.count_total_unique_less_grams = 0         # keep track of total number of unique n-1 grams\n",
        "        self.n_gram_count = {}                         # frequency mapping for n grams\n",
        "        self.count_total_non_unique_less_grams = 0     # keep track of total number of n grams\n",
        "        self.count_total_non_unique_n_grams = 0        # keep track of total number of n-1 grams\n",
        "        self.less_gram_count = {}                      # frequency mapping for n-1 grams\n",
        "        self.unique_words = {}                         # frequency mapping for unique tokens\n",
        "        self.unique_words_count = 0                    # keep track of total number of unique tokens\n",
        "\n",
        "        for sentence in self.corpus:\n",
        "            str = sentence.split()\n",
        "            # n grams handling\n",
        "            for i in range(len(str)-self.N+1):\n",
        "                self.count_total_non_unique_n_grams += 1\n",
        "                n_gram = str[i:i+self.N]\n",
        "                n = '' # store all n grams as string\n",
        "                for word in n_gram:\n",
        "                    n = n + word + ' '\n",
        "                n = n[:len(n)-1]                      \n",
        "                if n in self.n_gram_count:\n",
        "                    self.n_gram_count[n] += 1\n",
        "                else:\n",
        "                    self.n_gram_count[n] = 1\n",
        "                    self.count_total_unique_n_grams += 1\n",
        "                if str[i+self.N-1] not in self.unique_words:\n",
        "                    self.unique_words[str[i+self.N-1]] = 1\n",
        "                    self.unique_words_count += 1\n",
        "\n",
        "            # n-1 grams handling\n",
        "            for i in range(len(str)-self.N+1):\n",
        "                self.count_total_non_unique_less_grams += 1\n",
        "                n_gram = str[i:i+self.N-1]\n",
        "                n = '' # store all n-1 grams as string\n",
        "                for word in n_gram:\n",
        "                    n = n + word + ' '\n",
        "                n = n[:len(n)-1]\n",
        "                if n in self.less_gram_count:\n",
        "                    self.less_gram_count[n] += 1\n",
        "                else:\n",
        "                    self.less_gram_count[n] = 1\n",
        "                    self.count_total_unique_less_grams += 1\n",
        "\n",
        "    def perplexity(self, sentence):\n",
        "        '''\n",
        "            function to calculate perplexity of a sentence\n",
        "            i) laplace smoothing is applied for n-grams\n",
        "            ii) for unigram no smoothing is required \n",
        "                if word is not found it is treated as UNK\n",
        "            args:\n",
        "                sentence:: sentence for which perplexity(PP) has to be computed\n",
        "        '''\n",
        "        PP = 0\n",
        "        str = sentence.split()\n",
        "        if self.N == 1:\n",
        "            for i in range (len(str)):\n",
        "                # probability = count of word / vocabulary size (+1 to accomodate UNK)\n",
        "                PP = PP + log(self.n_gram_count.get(str[i],1)) - log(self.count_total_non_unique_n_grams+1)\n",
        "            # return perplexity\n",
        "            return (exp((-PP)/len(str))).real\n",
        "        for i in range(len(str)-self.N+1):\n",
        "            # determine n-1 gram \n",
        "            m_gram = str[i:i+self.N-1]\n",
        "            m = ''\n",
        "            for word in m_gram:\n",
        "                m = m + word + ' '\n",
        "            m = m[:len(m)-1]\n",
        "            # determine n gram\n",
        "            n = m + ' ' + str[i+self.N-1]\n",
        "            # probability = count of ngram + 1 / count of n-1gram + V (laplace smoothing)\n",
        "            PP = PP + log(self.n_gram_count.get(n,0)+1) - log(self.less_gram_count.get(m,0) + (self.unique_words_count))\n",
        "        # return perplexity\n",
        "        return (exp((-PP)/len(str))).real\n",
        "\n",
        "    def get_perplexity(self):\n",
        "        '''\n",
        "            function to calculate perplexity of test corpus\n",
        "            perplexity of a corpus = (product of probabilities of each sentence)^(1/corpus size)\n",
        "        '''\n",
        "        self.n_gram_model()\n",
        "        self.length_test_set = 0                         # keep track of test set size\n",
        "        for sentence in self.test_corpus:\n",
        "            self.length_test_set += len(sentence.split())\n",
        "        PP = 0\n",
        "        for sentence in self.test_corpus:\n",
        "            # convert perplexity back to log probability\n",
        "            PP += log(self.perplexity(sentence))*(len(sentence.split())-self.N+1-1)\n",
        "        PP = exp(PP/self.count_test_corpus)\n",
        "        # return perplexity\n",
        "        return (PP.real)\n",
        "\n",
        "    def generate_sentence(self, seed = '', max_length = 25):\n",
        "        '''\n",
        "            function to generate sentence based on the maximum probable n-gram\n",
        "            args:\n",
        "                seed:: the partial sequence\n",
        "                max_length:: maximum sequence length desirable\n",
        "        '''\n",
        "        self.n_gram_model() # get model parameters\n",
        "        # add start tokens in case words fall short for generation\n",
        "        m = len(seed.split())\n",
        "        sentence = seed\n",
        "        # add N-1 start tokens\n",
        "        sentence = ('<START> ' * (self.N - 1)) + seed\n",
        "        sentence = sentence.split()\n",
        "        # keep generating words until END token or length limit is reached\n",
        "        while sentence[len(sentence)-1] != '<END>' and (len(sentence)- self.N + 1 + m) < max_length:\n",
        "            # extract the n-1 gram\n",
        "            m_gram = ''\n",
        "            for i in range(len(sentence)-self.N+1,len(sentence)):\n",
        "                m_gram += sentence[i] + ' '\n",
        "            m_gram = m_gram[:len(m_gram)-1]\n",
        "            # determine the best candidate for next word based on minimum log PP score\n",
        "            PP_minima = -1e100\n",
        "            best_candidate = ''\n",
        "            for word in self.unique_words:\n",
        "                    n_gram = m_gram + ' ' + word\n",
        "                    # minimize PP score\n",
        "                    PP = log(self.n_gram_count.get(n_gram,0)+1) - log(self.less_gram_count.get(m_gram,0) + self.unique_words_count)\n",
        "                    PP = PP.real\n",
        "                    if PP > PP_minima:\n",
        "                        best_candidate = word\n",
        "                        PP_minima = PP\n",
        "            sentence.append(best_candidate)\n",
        "        if sentence[len(sentence)-1] != '<END>':\n",
        "            sentence.append('<END>')\n",
        "        # print the most probable sequence\n",
        "        sentence = sentence[self.N - 1:len(sentence) - 1]\n",
        "        print(' '.join([str(elem) for elem in sentence]))"
      ],
      "metadata": {
        "id": "yj_BJ2nO2E3e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train and test the model"
      ],
      "metadata": {
        "id": "Q7hQ2u8xVvEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model and check its perplexity score on the test dataset\n",
        "for i in range(1, 4):\n",
        "    train = open('train.txt').read()\n",
        "    test = open('test.txt').read()\n",
        "    sentences = train.splitlines()\n",
        "    test_sentences = test.splitlines()\n",
        "    object = NGramModel(i, sentences, test_sentences)\n",
        "    perplexity = object.get_perplexity()\n",
        "    print('perplexity of {}-gram model is:: {}'.format(i, perplexity))\n",
        "    del object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXWRqL5aTYU2",
        "outputId": "46bfe4cf-11c1-445d-d4ca-ac9a4e24f236"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of 1-gram model is:: 777.7048283969436\n",
            "perplexity of 2-gram model is:: 580.9471390295671\n",
            "perplexity of 3-gram model is:: 1291.848782371178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test the generate sentence method for bigram onwards: \n",
        "\n",
        "(it can be noticed that the complexity of the sentences are more for greater values of N)"
      ],
      "metadata": {
        "id": "WUTEd28wVmI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the most likely sentence based on N-gram models\n",
        "for i in range(2, 6):\n",
        "    train = open('train.txt').read()\n",
        "    test = open('test.txt').read()\n",
        "    sentences = train.splitlines()\n",
        "    test_sentences = test.splitlines()\n",
        "    object = NGramModel(i, sentences, test_sentences)\n",
        "    print('\\nsentence generation by {}-gram model::'.format(i))\n",
        "    object.generate_sentence('the sale', 20)\n",
        "    del object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_kr94jX5dUf",
        "outputId": "00b54a25-85ee-444e-f8d0-028839e5ae1b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence generation by 2-gram model::\n",
            "the sale of the company said\n",
            "\n",
            "sentence generation by 3-gram model::\n",
            "the sale of its common stock\n",
            "\n",
            "sentence generation by 4-gram model::\n",
            "the sale is part of a restructuring plan it expects pre tax operating loss of two cts a\n",
            "\n",
            "sentence generation by 5-gram model::\n",
            "the sale is part of a major program to divest several of its businesses representing about 200 mln\n"
          ]
        }
      ]
    }
  ]
}